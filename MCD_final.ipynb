{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MCD_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gfelekis/MSc-Dissertation/blob/master/MCD_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "1QS81BeEg6Tq",
        "colab": {}
      },
      "source": [
        "#@title Imports\n",
        "!pip install GPy\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.stats\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import os\n",
        "import GPy\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbz3rFUlEUrG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title UCI Datasets\n",
        "#Boston housing dataset\n",
        "np.random.seed(2)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\" --no-check-certificate \n",
        "data1 = pd.read_csv('housing.data', header=0, delimiter=\"\\s+\").values\n",
        "data1 = data1[np.random.permutation(np.arange(len(data1)))]\n",
        "\n",
        "# Concrete compressive dataset\n",
        "np.random.seed(2)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\" --no-check-certificate\n",
        "data2 = pd.read_excel('Concrete_Data.xls', header=0, delimiter=\"\\s+\").values\n",
        "data2 = data2[np.random.permutation(np.arange(len(data2)))]\n",
        "\n",
        "# Energy efficiency dataset\n",
        "np.random.seed(2)\n",
        "!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\" --no-check-certificate\n",
        "data3 = pd.read_excel('ENB2012_data.xlsx', header=0, delimiter=\"\\s+\").values\n",
        "data3 = data3[np.random.permutation(np.arange(len(data3)))]\n",
        "\n",
        "# Red wine dataset\n",
        "np.random.seed(2)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\" --no-check-certificate \n",
        "data4 = pd.read_csv('winequality-red.csv', header=1, delimiter=';').values\n",
        "data4 = data4[np.random.permutation(np.arange(len(data4)))]\n",
        "\n",
        "#Yacht dataset\n",
        "np.random.seed(2)\n",
        "!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\" --no-check-certificate \n",
        "data5 = pd.read_csv('yacht_hydrodynamics.data', header=1, delimiter='\\s+').values\n",
        "data5 = data5[np.random.permutation(np.arange(len(data5)))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGqAWMNFhi78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.manual_seed_all(999)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNSdSVnSorhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovd_IWAepALw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma) - 0.5*no_dim*np.log(2*np.pi)\n",
        "    \n",
        "    return - (log_coeff + exponent).sum()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr_H64y4pdWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MC_Dropout_Model_UCI(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units, drop_prob):\n",
        "        super(MC_Dropout_Model_UCI, self).__init__()\n",
        "        #torch.manual_seed(42)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.drop_prob = drop_prob\n",
        "        self.num_units = num_units\n",
        "\n",
        "        # network with two hidden and one output layer\n",
        "        if len(num_units) == 1:\n",
        "          self.layer1 = nn.Linear(input_dim, num_units[0])\n",
        "          self.layer2 = nn.Linear(num_units[0], 2*output_dim)\n",
        "        if len(num_units) == 2:\n",
        "          self.layer1 = nn.Linear(input_dim, num_units[0])\n",
        "          self.layer2 = nn.Linear(num_units[0], num_units[1])\n",
        "          self.layer3 = nn.Linear(num_units[1], 2*output_dim)\n",
        "        elif len(num_units) == 3:\n",
        "          self.layer1 = nn.Linear(input_dim, num_units[0])\n",
        "          self.layer2 = nn.Linear(num_units[0], num_units[1])\n",
        "          self.layer3 = nn.Linear(num_units[1], num_units[2])\n",
        "          self.layer4 = nn.Linear(num_units[2], 2*output_dim)\n",
        "\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)\n",
        "\n",
        "        if len(self.num_units) == 1:\n",
        "          x = self.layer1(x)\n",
        "          x = self.activation(x)\n",
        "          x = F.dropout(x, p=self.drop_prob, training=True)\n",
        "          x = self.layer2(x)\n",
        "\n",
        "        if len(self.num_units) == 2:\n",
        "          x = self.layer1(x)\n",
        "          x = self.activation(x)\n",
        "          x = F.dropout(x, p=self.drop_prob, training=True)\n",
        "\n",
        "          x = self.layer2(x)\n",
        "          x = self.activation(x)\n",
        "          x = F.dropout(x, p=self.drop_prob, training=True)\n",
        "\n",
        "          x = self.layer3(x)\n",
        "\n",
        "        elif len(self.num_units) == 3:\n",
        "          x = self.layer1(x)\n",
        "          x = self.activation(x)\n",
        "          x = F.dropout(x, p=self.drop_prob, training=True)\n",
        "\n",
        "          x = self.layer2(x)\n",
        "          x = self.activation(x)\n",
        "          x = F.dropout(x, p=self.drop_prob, training=True)\n",
        "\n",
        "          x = self.layer3(x)\n",
        "          x = self.activation(x)\n",
        "          x = F.dropout(x, p=self.drop_prob, training=True)\n",
        "\n",
        "          x = self.layer4(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def train_mc_dropout(data, drop_prob, n_splits, num_epochs, num_units, batch_size, learn_rate, weight_decay, log_every, num_samples):\n",
        "    #torch.manual_seed(42)\n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    in_dim = data.shape[1] - 1\n",
        "    train_logliks, test_logliks = [], []\n",
        "    train_rmses, test_rmses = [], []\n",
        "\n",
        "    for j, idx in enumerate(kf.split(data)):\n",
        "        num = j+1\n",
        "        print('SPLIT %d:' % num)\n",
        "        train_index, test_index = idx\n",
        "\n",
        "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
        "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
        "\n",
        "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
        "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
        "\n",
        "        x_train = (x_train - x_means)/x_stds\n",
        "        y_train = (y_train - y_means)/y_stds\n",
        "\n",
        "        x_test = (x_test - x_means)/x_stds\n",
        "        y_test = (y_test - y_means)/y_stds\n",
        "\n",
        "        net = MC_Dropout_Wrapper(network=MC_Dropout_Model_UCI(input_dim=in_dim, output_dim=1, num_units=num_units, drop_prob=drop_prob),\n",
        "                                 learn_rate=learn_rate, batch_size=batch_size, weight_decay=weight_decay)\n",
        "\n",
        "        fit_loss_train = np.zeros(num_epochs)\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "            loss = net.fit(x_train, y_train)\n",
        "                \n",
        "            if i % log_every == 0 or i == num_epochs - 1:\n",
        "                test_loss, rmse = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
        "                test_loss, rmse = test_loss.cpu().data.numpy(), rmse.cpu().data.numpy()\n",
        "\n",
        "                print('Epoch: %4d, Train loss: %6.3f Test loss: %6.3f RMSE: %.3f' %\n",
        "                      (i, loss.cpu().data.numpy()/len(x_train), test_loss/len(x_test), rmse*y_stds[0]))\n",
        "\n",
        "\n",
        "        train_loss, train_rmse = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
        "        test_loss, test_rmse = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
        "        \n",
        "        train_logliks.append((train_loss.cpu().data.numpy()/len(x_train) + np.log(y_stds)[0]))\n",
        "        test_logliks.append((test_loss.cpu().data.numpy()/len(x_test) + np.log(y_stds)[0]))\n",
        "\n",
        "        train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
        "        test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
        "\n",
        "\n",
        "    print('Train log. lik. = %6.3f +/- %6.3f' % (-np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
        "    print('Test  log. lik. = %6.3f +/- %6.3f' % (-np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
        "    print('Train RMSE      = %6.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
        "    print('Test  RMSE      = %6.3f +/- %6.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
        "    \n",
        "    rmses =  list(np.array(test_rmses).flatten())\n",
        "    print(\"Test LogLike for different folds: \", test_logliks)\n",
        "    print(\"Test RMSEs   for different folds: \", rmses)\n",
        "\n",
        "    metrics = {\"train_log_like_mean\": -np.array(train_logliks).mean(), \"train_log_like_var\": np.array(train_logliks).var()**0.5,\n",
        "               \"test_log_like_mean\": -np.array(test_logliks).mean(), \"test_log_like_var\": np.array(test_logliks).var()**0.5,\n",
        "               \"train_rmse_mean\": np.array(train_rmses).mean(), \"train_rmse_var\": np.array(train_rmses).var()**0.5,\n",
        "               \"test_rmse_mean\": np.array(test_rmses).mean(), \"test_rmse_var\":np.array(test_rmses).var()**0.5,\n",
        "               \"rmse_values\": list(np.array(test_rmses).flatten()),\n",
        "               \"loglik_values\": list(np.array(test_logliks).flatten()),\n",
        "               }\n",
        "\n",
        "    return net, metrics"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bA0eMK-pQAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MC_Dropout_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size , weight_decay):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = torch.optim.SGD(self.network.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        #torch.manual_seed(42)\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def get_loss_and_rmse(self, x, y, num_samples):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        means, stds = [], []\n",
        "        for i in range(num_samples):\n",
        "            output = self.network(x)\n",
        "            means.append(output[:, :1])\n",
        "            stds.append(output[:, 1:].exp())\n",
        "        \n",
        "        means, stds = torch.cat(means, dim=1), torch.cat(stds, dim=1)\n",
        "        mean = means.mean(dim=-1)[:, None]\n",
        "        std = ((means.var(dim=-1) + stds.mean(dim=-1)**2)**0.5)[:, None]\n",
        "        loss = self.loss_func(mean, y, std, 1)\n",
        "        \n",
        "        rmse = ((mean - y)**2).mean()**0.5\n",
        "\n",
        "        return loss.detach().cpu(), rmse.detach().cpu()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpg9g-sCKcro",
        "colab_type": "text"
      },
      "source": [
        "## RUN THE EXPERIMENTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdZ5w8R-6tMx",
        "colab_type": "text"
      },
      "source": [
        "# Regression on UCI data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfItkE1tKfsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up the access to drive - we'll be saving our logs there\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "# define paths for the different experiments\n",
        "mcd_path = \"/content/drive/My Drive/\"+\"new_mcd_logs.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SFwYAXW5g7xF",
        "colab": {}
      },
      "source": [
        "logs = []\n",
        "### some params\n",
        "n_splits = 30\n",
        "n_epochs = 200\n",
        "hidden   = 100\n",
        "models, log_metrics = [], []\n",
        "\n",
        "### alphas_experiment\n",
        "dataset = [data1, data2, data3, data4, data5] # list of all the datasets\n",
        "dataset_names = [\"Boston\",\"Concrete\", \"Energy\", \"Wine\", \"Yacht\"]\n",
        "hiddens = [3]\n",
        "\n",
        "for i, data in enumerate(dataset):\n",
        "  dataset_name = dataset_names[i]\n",
        "  for h in hiddens:\n",
        "    # run the training\n",
        "    num_units = [hidden for i in range(h)]\n",
        "    model, metric = train_mc_dropout(data=data, drop_prob=0.1, num_epochs=n_epochs, \n",
        "                                     n_splits=n_splits, num_units=num_units, batch_size =32, learn_rate=1e-4,\n",
        "                                     weight_decay=1e-1/len(data)**0.5, num_samples=20, log_every=50)\n",
        "    models.append(model)\n",
        "    #Â record to file: \n",
        "    log = {\"dataset\": dataset_name,\n",
        "           \"loss\": \"mc_dropout\",\n",
        "           \"alpha\": \"dropout_prob_\"+str(0.1),\n",
        "           \"n_layers\": h, \n",
        "           \"constant_hidden_size\": hidden, \n",
        "           \"metrics\": metric}\n",
        "    logs.append(log)\n",
        "    with open(mcd_path, \"a\") as f:\n",
        "      f.write(str(log)+\"\\n\")\n",
        "      print(log)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAYnEbzNsBex",
        "colab_type": "text"
      },
      "source": [
        "# Regression on GP ground truth\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VgFkpyifu4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_uncertainty_mcd(net, x_train, y_train):\n",
        "  samples = []\n",
        "  noises = []\n",
        "  for i in range(1000):\n",
        "      preds = net.network.forward(torch.linspace(-5, 5, 200).cuda()).cpu().data.numpy()\n",
        "      samples.append(preds[:, 0])\n",
        "      noises.append(np.exp(preds[:, 1]))\n",
        "      \n",
        "  samples = np.array(samples)\n",
        "  noises = np.array(noises)\n",
        "  means = (samples.mean(axis = 0)).reshape(-1)\n",
        "\n",
        "  aleatoric = (noises**2).mean(axis = 0)**0.5\n",
        "  epistemic = (samples.var(axis = 0)**0.5).reshape(-1)\n",
        "  total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
        "\n",
        "\n",
        "  c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
        "      '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
        "\n",
        "  plt.figure(figsize = (6, 5))\n",
        "  plt.style.use('default')\n",
        "  plt.scatter(x_train, y_train, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n",
        "  plt.fill_between(np.linspace(-5, 5, 200), means + aleatoric, means + total_unc, color = c[0], alpha = 0.3, label = 'Epistemic + Aleatoric')\n",
        "  plt.fill_between(np.linspace(-5, 5, 200), means - total_unc, means - aleatoric, color = c[0], alpha = 0.3)\n",
        "  plt.fill_between(np.linspace(-5, 5, 200), means - aleatoric, means + aleatoric, color = c[4], alpha = 0.4, label = 'Aleatoric')\n",
        "  plt.plot(np.linspace(-5, 5, 200), means, color = 'black', linewidth = 1)\n",
        "  plt.xlim([-5, 5])\n",
        "  plt.ylim([-5, 7])\n",
        "  plt.xlabel('$x$', fontsize=10)\n",
        "  plt.title('MC dropout', fontsize=10)\n",
        "  plt.tick_params(labelsize=10)\n",
        "  plt.xticks(np.arange(-4, 5, 2))\n",
        "  plt.yticks(np.arange(-4, 7, 2))\n",
        "  plt.gca().set_yticklabels([])\n",
        "  plt.gca().yaxis.grid(alpha=0.3)\n",
        "  plt.gca().xaxis.grid(alpha=0.3)\n",
        "  plt.savefig('mc_dropout_hetero.pdf', bbox_inches = 'tight')\n",
        "\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_EmfiSnWAon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_uncertainty_3row_mcd(nets, x, y):\n",
        "  fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
        "  fig.suptitle(\"MC Dropout h=1 | h=2 | h=3\")\n",
        "  for n, net in enumerate(nets):\n",
        "    samples, noises = [], []\n",
        "    for i in range(100):\n",
        "        preds = nets[n].network.forward(torch.linspace(-5, 5, 200).cuda()).cpu().data.numpy()\n",
        "        samples.append(preds[:, 0])\n",
        "        noises.append(preds[:, 1])\n",
        "\n",
        "    samples = np.array(samples)\n",
        "    means = (samples.mean(axis = 0)).reshape(-1)\n",
        "    noises = np.array(noises)\n",
        "\n",
        "    aleatoric = (noises**2).mean(axis = 0)**0.5\n",
        "    epistemic = samples.var(axis = 0)**0.5\n",
        "    total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
        "\n",
        "    c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
        "        '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
        "\n",
        "    ax[n].scatter(x_train, y_train, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n",
        "    ax[n].fill_between(np.linspace(-5, 5, 200), means + aleatoric, means + total_unc, color = c[0], alpha = 0.3, label = 'Epistemic + Aleatoric')\n",
        "    ax[n].fill_between(np.linspace(-5, 5, 200), means - total_unc, means - aleatoric, color = c[0], alpha = 0.3)\n",
        "    ax[n].fill_between(np.linspace(-5, 5, 200), means - aleatoric, means + aleatoric, color = c[4], alpha = 0.4, label = 'Aleatoric')\n",
        "    ax[n].plot(np.linspace(-5, 5, 200), means, color = 'black', linewidth = 1)\n",
        "    ax[n].set_xlim([-5, 5])\n",
        "    ax[n].set_ylim([-5, 7])\n",
        "    ax[n].set_xlabel('$x$', fontsize=10)\n",
        "    ax[n].set_title(\"h = \"+str(n+1), fontsize=10)\n",
        "    ax[n].tick_params(labelsize=10)\n",
        "    ax[n].set_xticks(np.arange(-4, 5, 2))\n",
        "    ax[n].set_yticks(np.arange(-4, 7, 2))\n",
        "    plt.gca().set_yticklabels([])\n",
        "    ax[n].grid(alpha=0.3)\n",
        "\n",
        "  plt.savefig('mcd_hetero.pdf', bbox_inches = 'tight')\n",
        "  plt.show()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOde8_JRpm2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2)\n",
        "no_points = 400\n",
        "lengthscale = 1\n",
        "variance = 1.0\n",
        "sig_noise = 0.3\n",
        "x = np.random.uniform(-3, 3, no_points)[:, None]\n",
        "x.sort(axis = 0)\n",
        "\n",
        "\n",
        "k = GPy.kern.RBF(input_dim=1, variance=variance)#, lengthscale=lengthscale)\n",
        "C = k.K(x, x) + np.eye(no_points)*(x + 2)**2*sig_noise**2\n",
        "\n",
        "y = np.random.multivariate_normal(np.zeros((no_points)), C)[:, None]\n",
        "y = (y - y.mean())\n",
        "x_train = x[75:325]\n",
        "y_train = y[75:325]\n",
        "\n",
        "num_epochs, batch_size = 2000, len(x_train)\n",
        "\n",
        "#h_nets = []\n",
        "for num_units in [[200], [200, 300], [200, 300, 200]]: \n",
        "  net = MC_Dropout_Wrapper(network=MC_Dropout_Model_UCI(input_dim=1, output_dim=1, num_units=num_units, drop_prob=0.5),\n",
        "                          learn_rate=1e-4, batch_size=batch_size, weight_decay=1e-2)\n",
        "\n",
        "  fit_loss_train = np.zeros(num_epochs)\n",
        "  best_net, best_loss = None, float('inf')\n",
        "  nets, losses = [], []\n",
        "\n",
        "  for i in range(num_epochs): # \n",
        "      \n",
        "      loss = net.fit(x_train, y_train)\n",
        "      \n",
        "      if i % 100 == 0:\n",
        "          print('Epoch: %4d, Train loss = %7.3f' % (i, loss.cpu().data.numpy()/batch_size))\n",
        "  #h_nets.append(copy.copy(net))\n",
        "\n",
        "  plot_uncertainty_mcd(net, x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9FfAbOD5m_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}