{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGLD_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gfelekis/MSc-Dissertation/blob/master/SGLD_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "1QS81BeEg6Tq",
        "colab": {}
      },
      "source": [
        "#@title Imports\n",
        "!pip install GPy\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.stats\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import os\n",
        "import GPy\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbz3rFUlEUrG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title UCI Datasets\n",
        "#Boston housing dataset\n",
        "np.random.seed(2)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\" --no-check-certificate \n",
        "data1 = pd.read_csv('housing.data', header=0, delimiter=\"\\s+\").values\n",
        "data1 = data1[np.random.permutation(np.arange(len(data1)))]\n",
        "\n",
        "# Concrete compressive dataset\n",
        "np.random.seed(2)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\" --no-check-certificate\n",
        "data2 = pd.read_excel('Concrete_Data.xls', header=0, delimiter=\"\\s+\").values\n",
        "data2 = data2[np.random.permutation(np.arange(len(data2)))]\n",
        "\n",
        "# Energy efficiency dataset\n",
        "np.random.seed(2)\n",
        "!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\" --no-check-certificate\n",
        "data3 = pd.read_excel('ENB2012_data.xlsx', header=0, delimiter=\"\\s+\").values\n",
        "data3 = data3[np.random.permutation(np.arange(len(data3)))]\n",
        "\n",
        "# Red wine dataset\n",
        "np.random.seed(2)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\" --no-check-certificate \n",
        "data4 = pd.read_csv('winequality-red.csv', header=1, delimiter=';').values\n",
        "data4 = data4[np.random.permutation(np.arange(len(data4)))]\n",
        "\n",
        "#Yacht dataset\n",
        "np.random.seed(2)\n",
        "!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\" --no-check-certificate \n",
        "data5 = pd.read_csv('yacht_hydrodynamics.data', header=1, delimiter='\\s+').values\n",
        "data5 = data5[np.random.permutation(np.arange(len(data5)))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGqAWMNFhi78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.manual_seed_all(999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1S5kt0omQ-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK1r-6p04Kv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Langevin_SGD(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr, weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        super(Langevin_SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            \n",
        "            weight_decay = group['weight_decay']\n",
        "            \n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                \n",
        "                if len(p.shape) == 1 and p.shape[0] == 1:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                    \n",
        "                else:\n",
        "                    if weight_decay != 0:\n",
        "                        d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                    unit_noise = Variable(p.data.new(p.size()).normal_())\n",
        "\n",
        "                    p.data.add_(-group['lr'], 0.5*d_p + unit_noise/group['lr']**0.5)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va8V78eFFsc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma)\n",
        "    \n",
        "    return - (log_coeff + exponent).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ASGi2Ecx5G-F",
        "colab": {}
      },
      "source": [
        "class Langevin_Layer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Langevin_Layer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.weights = nn.Parameter(torch.Tensor(self.input_dim, self.output_dim).uniform_(-0.01, 0.01))\n",
        "        self.biases = nn.Parameter(torch.Tensor(self.output_dim).uniform_(-0.01, 0.01))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        return torch.mm(x, self.weights) + self.biases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caJqaR1eGWEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_ensemble(x, y, ensemble):\n",
        "    x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "    means, stds = [], []\n",
        "    for net in ensemble:\n",
        "        output = net(x)\n",
        "        means.append(output[:, :1, None])\n",
        "        stds.append(output[:, 1:, None].exp())\n",
        "            \n",
        "    means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
        "    mean = means.mean(dim=2)\n",
        "    std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
        "    loss = log_gaussian_loss(mean, y, std, 1)/len(x)\n",
        "    \n",
        "    rmse = ((mean - y)**2).mean()**0.5\n",
        "\n",
        "    return loss, rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oAYelw3B5G-K",
        "colab": {}
      },
      "source": [
        "class Langevin_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size, no_batches, weight_decay):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.no_batches = no_batches\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = Langevin_SGD(self.network.parameters(), lr=self.learn_rate, weight_decay=weight_decay)\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def test_loss(self, x, y):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqs5AazDj93D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SGLD_Model_UCI(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units):\n",
        "        super(SGLD_Model_UCI, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_units = num_units\n",
        "      \n",
        "        # network with two hidden and one output layer\n",
        "        if len(num_units) == 1:\n",
        "          self.layer1 = Langevin_Layer(input_dim, num_units[0])\n",
        "          self.layer2 = Langevin_Layer(num_units[0], 2*output_dim)\n",
        "        if len(num_units) == 2:\n",
        "          self.layer1 = Langevin_Layer(input_dim, num_units[0])\n",
        "          self.layer2 = Langevin_Layer(num_units[0], num_units[1])\n",
        "          self.layer3 = Langevin_Layer(num_units[1], 2*output_dim)\n",
        "        elif len(num_units) == 3:\n",
        "          self.layer1 = Langevin_Layer(input_dim, num_units[0])\n",
        "          self.layer2 = Langevin_Layer(num_units[0], num_units[1])\n",
        "          self.layer3 = Langevin_Layer(num_units[1], num_units[2])\n",
        "          self.layer4 = Langevin_Layer(num_units[2], 2*output_dim)\n",
        "\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)\n",
        "\n",
        "        if len(self.num_units) == 1:\n",
        "          x = self.layer1(x)\n",
        "          x = self.activation(x)\n",
        "          x = self.layer2(x)\n",
        "\n",
        "        if len(self.num_units) == 2:\n",
        "          x = self.layer1(x)\n",
        "          x = self.activation(x)\n",
        "\n",
        "          x = self.layer2(x)\n",
        "          x = self.activation(x)\n",
        "\n",
        "          x = self.layer3(x)\n",
        "\n",
        "        elif len(self.num_units) == 3:\n",
        "          x = self.layer1(x)\n",
        "          x = self.activation(x)\n",
        "\n",
        "          x = self.layer2(x)\n",
        "          x = self.activation(x)\n",
        "\n",
        "          x = self.layer3(x)\n",
        "          x = self.activation(x)\n",
        "\n",
        "          x = self.layer4(x)\n",
        "                \n",
        "        return x\n",
        "\n",
        "\n",
        "def train_sgld(data, n_splits, burn_in, mix_time, num_nets, num_units, learn_rate, weight_decay, log_every):\n",
        "    #torch.manual_seed(42)\n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    in_dim = data.shape[1] - 1\n",
        "    train_logliks, test_logliks = [], []\n",
        "    train_rmses, test_rmses = [], []\n",
        "\n",
        "    for j, idx in enumerate(kf.split(data)):\n",
        "        print('FOLD %d:' % j)\n",
        "        train_index, test_index = idx\n",
        "\n",
        "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
        "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
        "\n",
        "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
        "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
        "\n",
        "        x_train = (x_train - x_means)/x_stds\n",
        "        y_train = (y_train - y_means)/y_stds\n",
        "\n",
        "        x_test = (x_test - x_means)/x_stds\n",
        "        y_test = (y_test - y_means)/y_stds\n",
        "\n",
        "        net = Langevin_Wrapper(network=SGLD_Model_UCI(input_dim=in_dim, output_dim=1, num_units=num_units),\n",
        "                               learn_rate=learn_rate, batch_size=250, no_batches=1, weight_decay=weight_decay)\n",
        "\n",
        "        nets, losses = [], []\n",
        "        num_epochs = burn_in + mix_time*num_nets + 1\n",
        "        fit_loss_train = np.zeros(num_epochs)\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "            loss = net.fit(x_train, y_train)\n",
        "\n",
        "            if i % mix_time == 0 and i > burn_in:\n",
        "                nets.append(copy.deepcopy(net.network))\n",
        "                \n",
        "            if i % log_every == 0 or i == num_epochs - 1:\n",
        "                test_loss = net.test_loss(x_test, y_test).cpu().data.numpy()\n",
        "\n",
        "                if len(nets) > 10: ensemble_loss, rmse = eval_ensemble(x_test, y_test, nets)\n",
        "                else: ensemble_loss, rmse = float('nan'), float('nan')\n",
        "\n",
        "                print('Epoch: %4d, Train loss: %6.3f Test loss: %6.3f Ensemble loss: %6.3f RMSE: %.3f Num. networks: %2d' %\n",
        "                      (i, loss.cpu().data.numpy()/len(x_train), test_loss/len(x_test), ensemble_loss, rmse*y_stds[0], len(nets)))\n",
        "\n",
        "\n",
        "        train_loss, train_rmse = eval_ensemble(x_train, y_train, nets)\n",
        "        test_loss, test_rmse = eval_ensemble(x_test, y_test, nets)\n",
        "\n",
        "        train_logliks.append(-(train_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "        test_logliks.append(-(test_loss.cpu().data.numpy() + np.log(y_stds)[0]))\n",
        "\n",
        "        train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
        "        test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
        "\n",
        "\n",
        "    print('Train log. lik. = %6.3f +/- %6.3f' % (np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
        "    print('Test  log. lik. = %6.3f +/- %6.3f' % (np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
        "    print('Train RMSE      = %6.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
        "    print('Test  RMSE      = %6.3f +/- %6.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
        "    \n",
        "    rmses =  list(np.array(test_rmses).flatten())\n",
        "    print(\"Test LogLike for different folds: \", test_logliks)\n",
        "    print(\"Test RMSEs   for different folds: \", rmses)\n",
        "\n",
        "    metrics = {\"train_log_like_mean\": -np.array(train_logliks).mean(), \"train_log_like_var\": np.array(train_logliks).var()**0.5,\n",
        "               \"test_log_like_mean\": -np.array(test_logliks).mean(), \"test_log_like_var\": np.array(test_logliks).var()**0.5,\n",
        "               \"train_rmse_mean\": np.array(train_rmses).mean(), \"train_rmse_var\": np.array(train_rmses).var()**0.5,\n",
        "               \"test_rmse_mean\": np.array(test_rmses).mean(), \"test_rmse_var\":np.array(test_rmses).var()**0.5,\n",
        "               \"rmse_values\": list(np.array(test_rmses).flatten()),\n",
        "               \"loglik_values\": list(np.array(test_logliks).flatten()),\n",
        "               }\n",
        "\n",
        "    return nets, metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_UmVONkO3BR",
        "colab_type": "text"
      },
      "source": [
        "## RUN THE EXPERIMENTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6JVjuTx6n35",
        "colab_type": "text"
      },
      "source": [
        "# Regression on UCI data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgzXnGL3O5K_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up the access to drive - we'll be saving our logs there\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "# define paths for the different experiments\n",
        "langevin_path = \"/content/drive/My Drive/\"+\"new_langevin_logs.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3K6_cDQPCEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logs = []\n",
        "### some params\n",
        "n_splits = 30\n",
        "#n_epochs = 100\n",
        "hidden   = 100\n",
        "models, log_metrics = [], []\n",
        "\n",
        "dataset = [data1, data2, data3, data4, data5] # list of all the datasets\n",
        "dataset_names = [\"Boston\", \"Concrete\", \"Energy\", \"Wine\", \"Yacht\"]\n",
        "hiddens = [1,2,3]\n",
        "\n",
        "for i, data in enumerate(dataset):\n",
        "  dataset_name = dataset_names[i]\n",
        "  for h in hiddens:\n",
        "    # run the training\n",
        "    num_units = [hidden for i in range(h)]\n",
        "    model, metric  = train_sgld(data=data, n_splits=n_splits, burn_in=1000, mix_time=100, num_nets=50, \n",
        "                                num_units=num_units, learn_rate=1e-2/len(data), weight_decay=1, log_every=500)\n",
        "    models.append(model)\n",
        "    #Â record to file: \n",
        "    log = {\"dataset\": dataset_name,\n",
        "           \"loss\": \"sgld\",\n",
        "           \"alpha\": \"sgld\",\n",
        "           \"n_layers\": h, \n",
        "           \"constant_hidden_size\": hidden, \n",
        "           \"metrics\": metric}\n",
        "    logs.append(log)\n",
        "    with open(langevin_path, \"a\") as f:\n",
        "      f.write(str(log)+\"\\n\")\n",
        "      print(log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTZhg6tp6JNb",
        "colab_type": "text"
      },
      "source": [
        "# Regression on GP ground truth\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvP-HbNH_Uen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_uncertainty_sgld(nets, x_train, y_train):\n",
        "  print(\"Using %d networks for prediction\" % len(nets))\n",
        "  samples = []\n",
        "  noises = []\n",
        "  for network in nets:\n",
        "      preds = network.forward(torch.linspace(-5, 5, 200).cuda()).cpu().data.numpy()\n",
        "      samples.append(preds[:, 0])\n",
        "      noises.append(np.exp(preds[:, 1]))\n",
        "      \n",
        "  samples = np.array(samples)\n",
        "  means = (samples.mean(axis = 0)).reshape(-1)\n",
        "\n",
        "  noises = np.array(noises)\n",
        "  aleatoric = (noises**2).mean(axis = 0)**0.5\n",
        "  epistemic = samples.var(axis = 0)**0.5\n",
        "  total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
        "\n",
        "  c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
        "      '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
        "\n",
        "  plt.figure(figsize = (6, 5))\n",
        "  plt.style.use('default')\n",
        "  plt.scatter(x_train, y_train, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n",
        "  plt.fill_between(np.linspace(-5, 5, 200), means + aleatoric, means + total_unc, color = c[0], alpha = 0.3, label = 'Epistemic + Aleatoric')\n",
        "  plt.fill_between(np.linspace(-5, 5, 200), means - total_unc, means - aleatoric, color = c[0], alpha = 0.3)\n",
        "  plt.fill_between(np.linspace(-5, 5, 200), means - aleatoric, means + aleatoric, color = c[4], alpha = 0.4, label = 'Aleatoric')\n",
        "  plt.plot(np.linspace(-5, 5, 200), means, color = 'black', linewidth = 1)\n",
        "  plt.xlim([-5, 5])\n",
        "  plt.ylim([-5, 7])\n",
        "  plt.xlabel('$x$', fontsize=10)\n",
        "  plt.title('SGLD', fontsize=10)\n",
        "  plt.tick_params(labelsize=10)\n",
        "  plt.xticks(np.arange(-4, 5, 2))\n",
        "  plt.yticks(np.arange(-4, 7, 2))\n",
        "  plt.gca().set_yticklabels([])\n",
        "  plt.gca().yaxis.grid(alpha=0.3)\n",
        "  plt.gca().xaxis.grid(alpha=0.3)\n",
        "  plt.savefig('sgld_hetero.pdf', bbox_inches = 'tight')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avUkC8wBTgRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_uncertainty_3row_sgld(net_container, x, y):\n",
        "  fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
        "  fig.suptitle(\"SGLD h=1 | h=2 | h=3\")\n",
        "  \n",
        "  for n, nets in enumerate(net_container):\n",
        "    samples, noises = [], []\n",
        "    for network in nets:\n",
        "        preds = network.forward(torch.linspace(-5, 5, 200).cuda()).cpu().data.numpy()\n",
        "        samples.append(preds[:, 0])\n",
        "        noises.append(preds[:, 1])\n",
        "\n",
        "    samples = np.array(samples)\n",
        "    means = (samples.mean(axis = 0)).reshape(-1)\n",
        "    noises = np.array(noises)\n",
        "\n",
        "    aleatoric = (noises**2).mean(axis = 0)**0.5\n",
        "    epistemic = samples.var(axis = 0)**0.5\n",
        "    total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
        "\n",
        "    c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
        "        '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
        "\n",
        "    ax[n].scatter(x_train, y_train, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n",
        "    ax[n].fill_between(np.linspace(-5, 5, 200), means + aleatoric, means + total_unc, color = c[0], alpha = 0.3, label = 'Epistemic + Aleatoric')\n",
        "    ax[n].fill_between(np.linspace(-5, 5, 200), means - total_unc, means - aleatoric, color = c[0], alpha = 0.3)\n",
        "    ax[n].fill_between(np.linspace(-5, 5, 200), means - aleatoric, means + aleatoric, color = c[4], alpha = 0.4, label = 'Aleatoric')\n",
        "    ax[n].plot(np.linspace(-5, 5, 200), means, color = 'black', linewidth = 1)\n",
        "    ax[n].set_xlim([-5, 5])\n",
        "    ax[n].set_ylim([-5, 7])\n",
        "    ax[n].set_xlabel('$x$', fontsize=10)\n",
        "    ax[n].set_title(\"h = \"+str(n+1), fontsize=10)\n",
        "    ax[n].tick_params(labelsize=10)\n",
        "    ax[n].set_xticks(np.arange(-4, 5, 2))\n",
        "    ax[n].set_yticks(np.arange(-4, 7, 2))\n",
        "    plt.gca().set_yticklabels([])\n",
        "    ax[n].grid(alpha=0.3)\n",
        "\n",
        "  plt.savefig('sgld_hetero.pdf', bbox_inches = 'tight')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym6HBK-s8GnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2)\n",
        "no_points = 400\n",
        "lengthscale = 1\n",
        "variance = 1.0\n",
        "sig_noise = 0.3\n",
        "x = np.random.uniform(-3, 3, no_points)[:, None]\n",
        "x.sort(axis=0)\n",
        "\n",
        "\n",
        "k = GPy.kern.RBF(input_dim=1, variance=variance, lengthscale=lengthscale)\n",
        "C = k.K(x, x) + np.eye(no_points)*(x + 2)**2*sig_noise**2\n",
        "\n",
        "y = np.random.multivariate_normal(np.zeros((no_points)), C)[:, None]\n",
        "y = (y - y.mean())\n",
        "x_train = x[75:325]\n",
        "y_train = y[75:325]\n",
        "\n",
        "\n",
        "best_net, best_loss = None, float('inf')\n",
        "num_nets, nets, losses = 50, [], []\n",
        "mix_epochs, burnin_epochs = 100, 2000\n",
        "num_epochs = mix_epochs*num_nets + burnin_epochs + 1\n",
        "\n",
        "nets_container = []\n",
        "batch_size, nb_train = len(x_train), len(x_train)\n",
        "for num_units in [[200], [200, 200], [200, 200, 200]]:\n",
        "  nets = []\n",
        "  net  = Langevin_Wrapper(network=SGLD_Model_UCI(input_dim=1, output_dim=1, num_units=num_units),\n",
        "                        learn_rate=1e-4, batch_size=batch_size, no_batches=1, weight_decay=20)\n",
        "\n",
        "  for i in range(num_epochs):  #num_epochs\n",
        "    loss = net.fit(x_train, y_train)\n",
        "    \n",
        "    if i % mix_epochs == 0:\n",
        "        print('Epoch: %4d, Train loss = %8.3f' % (i, loss.cpu().data.numpy()))\n",
        "          \n",
        "    if i % 100 == 0 and i > burnin_epochs: \n",
        "      nets.append(copy.deepcopy(net.network))\n",
        "  plot_uncertainty_sgld(nets, x_train, y_train)\n",
        "  # nets_container.append(copy.deepcopy(nets))\n",
        "# plot_uncertainty_3row_sgld(nets_container, x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNv-aRWz50mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}